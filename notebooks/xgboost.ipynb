{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "# import dask.dataframe as dd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For making pretty LaTeX plots\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.figsize\": (8, 6),\n",
    "    \"figure.dpi\": 100,\n",
    "    \"savefig.dpi\": 200,\n",
    "    \"savefig.format\": \"png\",\n",
    "    \"savefig.transparent\": True,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.color\": \"0.8\",\n",
    "    \"image.cmap\": \"Blues\",\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"lines.markersize\": 6,\n",
    "    \"text.usetex\": True, \"mathtext.fontset\": \"cm\",\n",
    "    \"pgf.preamble\": r\"\\usepackage[utf8]{inputenc}\\usepackage[T1]{fontenc}\\usepackage{cmbright}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.read_csv('../data/preprocessed/engineered_training_set.csv')\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist of prop_country_id\n",
    "count = data['prop_country_id'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(count['prop_country_id'], count['count'])\n",
    "plt.xlabel('prop_country_id')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of prop_country_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is very messy, but essentially, all you need to do is choose a number of partitions, after which the variable `partition_list` will be a list of np.arrays, each of which are the `prop_country_id`'s for one partitions. Then you can train a model on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 12\n",
    "partitions_list = [np.array([219])]\n",
    "countries = count['prop_country_id'].to_numpy()\n",
    "indx = countries != 219\n",
    "countries = countries[indx]\n",
    "count_array = count['count'].to_numpy()\n",
    "\n",
    "\n",
    "# We take a cumsum, and get the indices at which we should split the data, sometimes we get empty partitions\n",
    "count_array = count_array[indx]\n",
    "cum_sum = np.cumsum(count_array)\n",
    "total = cum_sum[-1]\n",
    "partition_size = total // partitions\n",
    "for i in range(0, partitions):\n",
    "    idx_min = np.argmax(cum_sum >= partition_size * i)\n",
    "    idx_max = np.argmax(cum_sum > partition_size * (i + 1))\n",
    "    if i == partitions - 1:\n",
    "        partitions_list.append(countries[idx_min:])\n",
    "    else:\n",
    "        partitions_list.append(countries[idx_min:idx_max])\n",
    "\n",
    "# For printing and getting all the empty partitions\n",
    "counts = 0\n",
    "zeros = []\n",
    "for i, part in enumerate(partitions_list):\n",
    "    countries = count.filter(count['prop_country_id'].is_in(part))\n",
    "    part_count = countries['count']\n",
    "    counts += sum(part_count)\n",
    "    if i < 9:\n",
    "        print(f'Count for partion  {i+1}: {sum(part_count)}')\n",
    "    else:\n",
    "        print(f'Count for partion {i+1}: {sum(part_count)}')\n",
    "\n",
    "    if sum(part_count) == 0:\n",
    "        zeros.append(0)\n",
    "    else:\n",
    "        zeros.append(1)\n",
    "\n",
    "# Remove empty partitions\n",
    "partitions_list = [x for i, x in enumerate(partitions_list) if zeros[i] == 1]\n",
    "\n",
    "assert counts == sum(count['count']), f'Expected {sum(count[\"count\"])} but got {counts}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_set(data, partitions_list):\n",
    "    data_partitions = []\n",
    "    for part in partitions_list:\n",
    "        data_partitions.append(data.loc[data['prop_country_id'].isin(part)])\n",
    "    return data_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort(['srch_id', 'booking_bool', 'click_bool'], descending=[False, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert polars DataFrames to pandas DataFrames\n",
    "data_pd = data.to_pandas()\n",
    "# replace all NULL values with np.nan\n",
    "data_pd = data_pd.replace('NULL', np.nan)\n",
    "ranking_pd = data_pd[['srch_id', 'prop_id']]\n",
    "\n",
    "# Convert object columns to appropriate data types\n",
    "object_columns = data_pd.select_dtypes(include=['object']).columns\n",
    "data_pd[object_columns] = data_pd[object_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data_pd.drop(['srch_id'], axis=1)\n",
    "y = ranking_pd['prop_id']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets based on srch_id\n",
    "srch_ids = data_pd['srch_id'].unique()\n",
    "train_srch_ids, test_srch_ids = train_test_split(srch_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create training and testing DataFrames\n",
    "train_data = data_pd[data_pd['srch_id'].isin(train_srch_ids)]\n",
    "test_data = data_pd[data_pd['srch_id'].isin(test_srch_ids)]\n",
    "\n",
    "\n",
    "split_train = split_data_set(train_data, partitions_list)\n",
    "split_test = split_data_set(test_data, partitions_list)\n",
    "split_full = split_data_set(data_pd, partitions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, params, num_boost, drop_cols):\n",
    "    # Create XGBoost DMatrix objects for training and testing\n",
    "    train_dmatrix = xg.DMatrix(\n",
    "        data.drop(drop_cols, axis=1).sample(frac=1),\n",
    "        label=data['prop_id'],\n",
    "        group=data['srch_id'].value_counts().sort_index().values\n",
    "    )\n",
    "    \n",
    "    return xg.train(params, train_dmatrix, num_boost_round=100)\n",
    "\n",
    "def evaluate_model(model, cdata, drop_cols):\n",
    "    data = cdata.copy()\n",
    "\n",
    "    # Capture the true order of prop_id before sorting\n",
    "\n",
    "    # Create XGBoost DMatrix object for testing\n",
    "    test_dmatrix = xg.DMatrix(\n",
    "        data.drop(drop_cols, axis=1),\n",
    "        group=data['srch_id'].value_counts().sort_index().values\n",
    "    )\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    test_pred = model.predict(test_dmatrix)\n",
    "    data['pred'] = test_pred\n",
    "\n",
    "    # Order the prop_ids based on the predictions\n",
    "    results = data.sort_values(['srch_id', 'pred'], ascending=[True, False])[['srch_id', 'prop_id']]\n",
    "    \n",
    "\n",
    "    grouped = results.groupby('srch_id')['prop_id'].apply(list)\n",
    "    \n",
    "    return grouped.to_frame()\n",
    "\n",
    "\n",
    "def eval_models_partions(models, split_test, drop_cols, ncdg=True):\n",
    "    df = pd.DataFrame()\n",
    "    for model, part in zip(models, split_test):\n",
    "        grouped = evaluate_model(model, part, drop_cols)\n",
    "        true = part.groupby('srch_id')['prop_id'].apply(list)\n",
    "        grouped['prop_id_true'] = true\n",
    "\n",
    "        if ncdg:\n",
    "            grouped['ndcg'] = grouped.apply(lambda x: ndcg_score([x['prop_id_true']], [x['prop_id']], k=5) if len(x['prop_id_true']) > 1 else None, axis=1)\n",
    "            print(\"NDCG Score: \", grouped['ndcg'].mean())\n",
    "        df = pd.concat([df, grouped])\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_models(split_train, params, num_boost, drop_cols):\n",
    "    models = []\n",
    "    for part in tqdm(split_train):\n",
    "        model = train_model(part, params, num_boost, drop_cols)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'rank:pairwise', # the objective, can also be rank:ndcg, but that is buggy\n",
    "    'lambdarank_pair_method': 'topk', # instead of looking at the mean, we look at the highest k\n",
    "    'lambdarank_num_pair_per_sample': 6, # set slightly higher than intended k\n",
    "    'eval_metric': 'ndcg',\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 8,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "drop_cols = ['srch_id','prop_id', 'booking_bool', 'gross_bookings_usd', 'position', 'click_bool']\n",
    "\n",
    "models = train_models(split_train, params, 120, drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['srch_id','prop_id', 'booking_bool', 'gross_bookings_usd', 'position', 'click_bool']\n",
    "grouped = eval_models_partions(models, split_test, drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training\n",
    "models_full = train_models(split_full, params, 120, drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "for i, model in enumerate(models_full):\n",
    "    model.save_model(f'models/model_{i}.json')\n",
    "# load the model\n",
    "models_full = [xg.Booster() for i in range(len(models_full))]\n",
    "[models_full[i].load_model(f'models/model_{i}.json') for i in range(len(models_full))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pl.read_csv('../data/preprocessed/engineered_test_set.csv')\n",
    "test_set = test_set.to_pandas()\n",
    "test_set = test_set.replace('NULL', np.nan)\n",
    "\n",
    "object_columns = test_set.select_dtypes(include=['object']).columns\n",
    "test_set[object_columns] = test_set[object_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "split_eval = split_data_set(test_set, partitions_list)\n",
    "\n",
    "assert sum([len(x) for x in split_eval]) == len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['srch_id','prop_id']\n",
    "grouped = eval_models_partions(models_full, split_eval, ncdg=False, drop_cols=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = grouped.explode('prop_id')\n",
    "submission['srch_id'] = submission.index\n",
    "submission = submission[['srch_id', 'prop_id']]\n",
    "\n",
    "assert len(submission) == len(test_set)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submit/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuff for single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_set_dmatrix = xg.DMatrix(test_set.drop(['srch_id'], axis=1), group=test_set['srch_id'].value_counts().sort_index().values)\n",
    "test_set['pred'] = model.predict(test_set_dmatrix)\n",
    "\n",
    "# same as earlier, without need for calculating the ndcg, so less steps\n",
    "submission = test_set.sort_values(['srch_id', 'pred'], ascending=[True, False])[['srch_id', 'prop_id']]\n",
    "submission.to_csv('submit/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pl.read_csv('../data/preprocessed/engineered_test_set.csv')\n",
    "shuffled_test_set = test_set.sort(['srch_id', 'price_per_person'], descending=[False, True])[['srch_id', 'prop_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_test_set.write_csv('submit/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# srch_ids = test_set['srch_id'].unique()\n",
    "\n",
    "# # Create an empty DataFrame to store the predictions\n",
    "# submission_df = pd.DataFrame(columns=['srch_id', 'prop_id'])\n",
    "\n",
    "# #  Unique srch_ids in the test set\n",
    "# srch_ids = test_set['srch_id'].unique()\n",
    "\n",
    "# # Pre-allocate a list to collect results\n",
    "# results = []\n",
    "\n",
    "# # Iterate over each srch_id and make predictions\n",
    "# for srch_id in tqdm(srch_ids):\n",
    "#     # Get the data for the current srch_id\n",
    "#     srch_data = test_set[test_set['srch_id'] == srch_id]\n",
    "\n",
    "#     # Create DMatrix for the current srch_id\n",
    "#     srch_dmatrix = xg.DMatrix(srch_data.drop(['srch_id'], axis=1))\n",
    "\n",
    "#     # Make predictions for the current srch_id\n",
    "#     srch_pred = model.predict(srch_dmatrix)\n",
    "\n",
    "#     # Get the corresponding prop_ids for the current srch_id\n",
    "#     srch_prop_ids = srch_data['prop_id'].values\n",
    "\n",
    "#     # Sort the prop_ids based on the predicted scores\n",
    "#     sorted_indices = np.argsort(srch_pred)[::-1]\n",
    "#     sorted_prop_ids = srch_prop_ids[sorted_indices]\n",
    "\n",
    "#     # Collect the results for the current srch_id\n",
    "#     results.append(pd.DataFrame({'srch_id': srch_id, 'prop_id': sorted_prop_ids}))\n",
    "\n",
    "# # Concatenate all results into a single DataFrame\n",
    "# submission_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "\n",
    "# # Check final submission DataFrame size\n",
    "# print(f\"Expected number of entries: {len(test_set)}\")\n",
    "# print(f\"Actual number of entries: {len(submission_df)}\")\n",
    "\n",
    "# # Save the submission DataFrame to a CSV file\n",
    "# submission_df.to_csv('submit/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputationalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
