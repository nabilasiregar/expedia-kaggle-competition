{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xg\n",
    "# import dask.dataframe as dd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For making pretty LaTeX plots\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.size\": 18,\n",
    "    \"axes.titlesize\": 18,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 12,\n",
    "    \"ytick.labelsize\": 12,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"figure.figsize\": (8, 6),\n",
    "    \"figure.dpi\": 100,\n",
    "    \"savefig.dpi\": 200,\n",
    "    \"savefig.format\": \"png\",\n",
    "    \"savefig.transparent\": True,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.linewidth\": 0.5,\n",
    "    \"grid.linestyle\": \"--\",\n",
    "    \"grid.color\": \"0.8\",\n",
    "    \"image.cmap\": \"Blues\",\n",
    "    \"lines.linewidth\": 1.5,\n",
    "    \"lines.markersize\": 6,\n",
    "    \"text.usetex\": True, \"mathtext.fontset\": \"cm\",\n",
    "    \"pgf.preamble\": r\"\\usepackage[utf8]{inputenc}\\usepackage[T1]{fontenc}\\usepackage{cmbright}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.read_csv('../data/preprocessed/engineered_training_set.csv')\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist of prop_country_id\n",
    "count = data['prop_country_id'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(count['prop_country_id'], count['count'])\n",
    "plt.xlabel('prop_country_id')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram of prop_country_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is very messy, but essentially, all you need to do is choose a number of partitions, after which the variable `partition_list` will be a list of np.arrays, each of which are the `prop_country_id`'s for one partitions. Then you can train a model on each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 12\n",
    "partitions_list = [np.array([219])]\n",
    "countries = count['prop_country_id'].to_numpy()\n",
    "indx = countries != 219\n",
    "countries = countries[indx]\n",
    "count_array = count['count'].to_numpy()\n",
    "\n",
    "\n",
    "# We take a cumsum, and get the indices at which we should split the data, sometimes we get empty partitions\n",
    "count_array = count_array[indx]\n",
    "cum_sum = np.cumsum(count_array)\n",
    "total = cum_sum[-1]\n",
    "partition_size = total // partitions\n",
    "for i in range(0, partitions):\n",
    "    idx_min = np.argmax(cum_sum >= partition_size * i)\n",
    "    idx_max = np.argmax(cum_sum > partition_size * (i + 1))\n",
    "    if i == partitions - 1:\n",
    "        partitions_list.append(countries[idx_min:])\n",
    "    else:\n",
    "        partitions_list.append(countries[idx_min:idx_max])\n",
    "\n",
    "# For printing and getting all the empty partitions\n",
    "counts = 0\n",
    "zeros = []\n",
    "for i, part in enumerate(partitions_list):\n",
    "    countries = count.filter(count['prop_country_id'].is_in(part))\n",
    "    part_count = countries['count']\n",
    "    counts += sum(part_count)\n",
    "    if i < 9:\n",
    "        print(f'Count for partion  {i+1}: {sum(part_count)}')\n",
    "    else:\n",
    "        print(f'Count for partion {i+1}: {sum(part_count)}')\n",
    "\n",
    "    if sum(part_count) == 0:\n",
    "        zeros.append(0)\n",
    "    else:\n",
    "        zeros.append(1)\n",
    "\n",
    "# Remove empty partitions\n",
    "partitions_list = [x for i, x in enumerate(partitions_list) if zeros[i] == 1]\n",
    "\n",
    "assert counts == sum(count['count']), f'Expected {sum(count[\"count\"])} but got {counts}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_set(data, partitions_list):\n",
    "    data_partitions = []\n",
    "    for part in partitions_list:\n",
    "        data_partitions.append(data.loc[data['prop_country_id'].isin(part)])\n",
    "    return data_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort(['srch_id', 'booking_bool', 'click_bool'], descending=[False, True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert polars DataFrames to pandas DataFrames\n",
    "data_pd = data.to_pandas()\n",
    "data_pd['weight'] = 5 * data_pd['booking_bool'] + data_pd['click_bool']\n",
    "# replace all NULL values with np.nan\n",
    "data_pd = data_pd.replace('NULL', np.nan)\n",
    "ranking_pd = data_pd[['srch_id', 'prop_id']]\n",
    "\n",
    "# Convert object columns to appropriate data types\n",
    "object_columns = data_pd.select_dtypes(include=['object']).columns\n",
    "data_pd[object_columns] = data_pd[object_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Split the data into features (X) and target (y)\n",
    "X = data_pd.drop(['srch_id'], axis=1)\n",
    "y = ranking_pd['prop_id']\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets based on srch_id\n",
    "srch_ids = data_pd['srch_id'].unique()\n",
    "train_srch_ids, test_srch_ids = train_test_split(srch_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create training and testing DataFrames\n",
    "train_data = data_pd[data_pd['srch_id'].isin(train_srch_ids)]\n",
    "test_data = data_pd[data_pd['srch_id'].isin(test_srch_ids)]\n",
    "\n",
    "\n",
    "split_train = split_data_set(train_data, partitions_list)\n",
    "split_test = split_data_set(test_data, partitions_list)\n",
    "split_full = split_data_set(data_pd, partitions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data, params, num_boost, drop_cols):\n",
    "    # Create XGBoost DMatrix objects for training and testing\n",
    "    train_dmatrix = xg.DMatrix(\n",
    "        data.drop(drop_cols, axis=1),\n",
    "        label=data['weight'],\n",
    "        qid=data['srch_id']\n",
    "    )\n",
    "    \n",
    "    return xg.train(params, train_dmatrix, num_boost_round=num_boost)\n",
    "\n",
    "\n",
    "def dcg_at_k(r, k, method=1):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 1:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        else:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k, method=1):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k, method) / dcg_max\n",
    "\n",
    "\n",
    "def calculate_grades(data):\n",
    "    data['grade'] = np.where(data['booking_bool'] == 1, 5,\n",
    "                             np.where(data['click_bool'] == 1, 1, 0))\n",
    "    return data\n",
    "\n",
    "\n",
    "def predict_and_evaluate(model, dmatrix, t_data, ndcg=True):\n",
    "    test_pred = model.predict(dmatrix)\n",
    "    results_data = t_data.copy()\n",
    "    results_data['pred'] = test_pred\n",
    "\n",
    "    # Assign grades based on booking and clicking\n",
    "    if ndcg:\n",
    "        results_data = calculate_grades(results_data)\n",
    "\n",
    "    # Sort predictions with highest probability first\n",
    "    ordered_results = results_data.sort_values(['srch_id', 'pred'], ascending=[True, False])\n",
    "    if ndcg:\n",
    "        grouped = ordered_results.groupby('srch_id')['grade'].apply(list).reset_index()\n",
    "\n",
    "        ndcg_scores = grouped['grade'].apply(lambda grades: ndcg_at_k(grades, 5))\n",
    "        mean_ndcg = ndcg_scores.mean()\n",
    "        print(f'Mean NDCG: {mean_ndcg}')\n",
    "    \n",
    "    return ordered_results[['srch_id', 'pred', 'prop_id']], mean_ndcg if ndcg else None\n",
    "\n",
    "\n",
    "def eval_models_partions(models, split_test, drop_cols, ndcg=True):\n",
    "    df = pd.DataFrame()\n",
    "    mean_ndcgs = []\n",
    "    for model, part in zip(models, split_test):\n",
    "        dmat = xg.DMatrix(\n",
    "            part.drop(drop_cols, axis=1), \n",
    "            group=part['srch_id'].value_counts().sort_index().values\n",
    ")\n",
    "        grouped, mean = predict_and_evaluate(model, dmat, part, ndcg=ndcg)\n",
    "\n",
    "        df = pd.concat([df, grouped])\n",
    "        if ndcg:\n",
    "            mean_ndcgs.append(mean*len(part))\n",
    "    return df, mean_ndcgs\n",
    "\n",
    "# Set XGBoost parameters\n",
    "config_= {\n",
    "    'objective': 'rank:pairwise', # the objective, can also be rank:ndcg, but that is buggy\n",
    "    'lambdarank_pair_method': 'topk', # instead of looking at the mean, we look at the highest k\n",
    "    'lambdarank_num_pair_per_sample': 6, # set slightly higher than intended k\n",
    "    'eval_metric': 'ndcg',\n",
    "    'eta': 0.13963013806537555,\n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.7038375178678972,\n",
    "    'colsample_bytree': 0.7015452447331039,\n",
    "    'seed': 42\n",
    "}\n",
    "config = {\n",
    "    'objective': 'rank:ndcg',\n",
    "    'eval_metric': 'ndcg@5',\n",
    "    'eta': 0.13963013806537555,\n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.7038375178678972,\n",
    "    'colsample_bytree': 0.7015452447331039,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "def train_models(split_train, params, num_boost, drop_cols):\n",
    "    models = []\n",
    "    for part in tqdm(split_train):\n",
    "        model = train_model(part, params, num_boost, drop_cols)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'objective': 'rank:ndcg',  \n",
    "        'eval_metric': 'ndcg',    \n",
    "        'learning_rate': 0.014,  \n",
    "        'ndcg_exp_gain': False,\n",
    "        'max_depth': 6,            # Shallow trees to prevent overfitting\n",
    "        'subsample': 0.92,         # Subsampling to prevent overfitting\n",
    "        'colsample_bytree': 0.78, \n",
    "        'seed': 42              \n",
    "    }\n",
    "\n",
    "drop_cols = ['srch_id','prop_id', 'booking_bool', 'gross_bookings_usd', 'position', 'click_bool', 'weight']\n",
    "\n",
    "models = train_models(split_train, config, 120, drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['srch_id','prop_id', 'booking_bool', 'gross_bookings_usd', 'position', 'click_bool', 'weight']\n",
    "grouped,means = eval_models_partions(models, split_test, drop_cols)\n",
    "\n",
    "print(f'Total Mean NDCG: {sum(means)/len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training\n",
    "models_full = train_models(split_full, config, 120, drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "\n",
    "for i, model in enumerate(models_full):\n",
    "    model.save_model(f'models/model_{i}.json')\n",
    "# load the model\n",
    "models_full = [xg.Booster() for i in range(len(partitions_list))]\n",
    "[models_full[i].load_model(f'models/model_{i}.json') for i in range(len(models_full))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pl.read_csv('../data/preprocessed/engineered_test_set.csv')\n",
    "test_set = test_set.to_pandas()\n",
    "test_set = test_set.replace('NULL', np.nan)\n",
    "\n",
    "object_columns = test_set.select_dtypes(include=['object']).columns\n",
    "test_set[object_columns] = test_set[object_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "split_eval = split_data_set(test_set, partitions_list)\n",
    "\n",
    "assert sum([len(x) for x in split_eval]) == len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['srch_id','prop_id']\n",
    "grouped, mean = eval_models_partions(models_full, split_eval, ndcg=False, drop_cols=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = grouped[['srch_id', 'prop_id']]\n",
    "\n",
    "assert len(submission) == len(test_set)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submit/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuff for single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "test_set_dmatrix = xg.DMatrix(test_set.drop(['srch_id'], axis=1), group=test_set['srch_id'].value_counts().sort_index().values)\n",
    "test_set['pred'] = model.predict(test_set_dmatrix)\n",
    "\n",
    "# same as earlier, without need for calculating the ndcg, so less steps\n",
    "submission = test_set.sort_values(['srch_id', 'pred'], ascending=[True, False])[['srch_id', 'prop_id']]\n",
    "submission.to_csv('submit/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_set = pl.read_csv('../data/preprocessed/engineered_test_set.csv')\n",
    "shuffled_test_set = test_set.sort(['srch_id', 'price_per_person'], descending=[False, True])[['srch_id', 'prop_id']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffled_test_set.write_csv('submit/submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputationalEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
